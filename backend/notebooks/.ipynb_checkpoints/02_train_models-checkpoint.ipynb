{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Symptoms Checker - Model Training\n",
    "\n",
    "This notebook trains and evaluates different ML models for symptom triage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "from src.config import TRIAGE_LEVELS, MODELS_DIR\n",
    "\n",
    "# Ensure models directory exists\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "df = pd.read_csv('../data/processed_symptom_cases.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nTriage distribution:\")\n",
    "print(df['triage_name'].value_counts())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "symptom_features = [\n",
    "    'fever', 'headache', 'cough', 'chest_pain', 'shortness_breath',\n",
    "    'nausea', 'vomiting', 'diarrhea', 'fatigue', 'dizziness',\n",
    "    'sore_throat', 'runny_nose', 'muscle_pain', 'abdominal_pain'\n",
    "]\n",
    "\n",
    "severity_features = ['pain_severity', 'duration_severity', 'intensity_severity']\n",
    "demographic_features = ['age_group', 'gender']\n",
    "other_features = ['symptom_count', 'text_length']\n",
    "\n",
    "# Filter features that exist in the dataset\n",
    "available_symptom_features = [f for f in symptom_features if f in df.columns]\n",
    "available_severity_features = [f for f in severity_features if f in df.columns]\n",
    "available_demographic_features = [f for f in demographic_features if f in df.columns]\n",
    "available_other_features = [f for f in other_features if f in df.columns]\n",
    "\n",
    "print(f\"Available symptom features: {available_symptom_features}\")\n",
    "print(f\"Available severity features: {available_severity_features}\")\n",
    "print(f\"Available demographic features: {available_demographic_features}\")\n",
    "print(f\"Available other features: {available_other_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix and labels\n",
    "feature_columns = (available_symptom_features + \n",
    "                  available_severity_features + \n",
    "                  available_other_features)\n",
    "\n",
    "# Handle categorical features\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Encode age_group if available\n",
    "if 'age_group' in df.columns:\n",
    "    age_encoder = LabelEncoder()\n",
    "    df_encoded['age_group_encoded'] = age_encoder.fit_transform(df['age_group'].fillna('unknown'))\n",
    "    feature_columns.append('age_group_encoded')\n",
    "\n",
    "# Encode gender if available\n",
    "if 'gender' in df.columns:\n",
    "    gender_encoder = LabelEncoder()\n",
    "    df_encoded['gender_encoded'] = gender_encoder.fit_transform(df['gender'].fillna('unknown'))\n",
    "    feature_columns.append('gender_encoded')\n",
    "\n",
    "# Prepare final feature matrix\n",
    "X_structured = df_encoded[feature_columns].fillna(0)\n",
    "X_text = df['cleaned_text'] if 'cleaned_text' in df.columns else df['complaint_text']\n",
    "y = df['triage_label']\n",
    "\n",
    "print(f\"Structured features shape: {X_structured.shape}\")\n",
    "print(f\"Text features count: {len(X_text)}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {feature_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Text-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for text-based models\n",
    "X_text_train, X_text_test, y_train, y_test = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_text_train)}\")\n",
    "print(f\"Test set size: {len(X_text_test)}\")\n",
    "print(f\"\\nTraining set distribution:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Fit and transform text data\n",
    "X_text_train_tfidf = tfidf.fit_transform(X_text_train)\n",
    "X_text_test_tfidf = tfidf.transform(X_text_test)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_text_train_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train text-based models\n",
    "text_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Linear SVM': LinearSVC(random_state=42, max_iter=2000)\n",
    "}\n",
    "\n",
    "text_results = {}\n",
    "\n",
    "print(\"Training text-based models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model in text_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_text_train_tfidf, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_text_test_tfidf)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_text_train_tfidf, y_train, cv=5)\n",
    "    \n",
    "    text_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "    print(f\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[TRIAGE_LEVELS[i] for i in sorted(TRIAGE_LEVELS.keys())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Structured Feature Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split structured data\n",
    "X_struct_train, X_struct_test, y_struct_train, y_struct_test = train_test_split(\n",
    "    X_structured, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_struct_train_scaled = scaler.fit_transform(X_struct_train)\n",
    "X_struct_test_scaled = scaler.transform(X_struct_test)\n",
    "\n",
    "print(f\"Structured training set shape: {X_struct_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train structured feature models\n",
    "struct_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression (Structured)': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "struct_results = {}\n",
    "\n",
    "print(\"Training structured feature models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model in struct_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Use scaled features for logistic regression, original for random forest\n",
    "    if 'Logistic' in name:\n",
    "        X_train_use = X_struct_train_scaled\n",
    "        X_test_use = X_struct_test_scaled\n",
    "    else:\n",
    "        X_train_use = X_struct_train\n",
    "        X_test_use = X_struct_test\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_use, y_struct_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_use)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_struct_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train_use, y_struct_train, cv=5)\n",
    "    \n",
    "    struct_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "    print(f\"Classification Report:\")\n",
    "    print(classification_report(y_struct_test, y_pred, target_names=[TRIAGE_LEVELS[i] for i in sorted(TRIAGE_LEVELS.keys())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = {**text_results, **struct_results}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for name, result in all_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Test Accuracy': result['accuracy'],\n",
    "        'CV Mean': result['cv_mean'],\n",
    "        'CV Std': result['cv_std']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Select best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model_result = all_results[best_model_name]\n",
    "best_model = best_model_result['model']\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "print(f\"Best accuracy: {best_model_result['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot accuracy comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(comparison_df)), comparison_df['Test Accuracy'])\n",
    "plt.xticks(range(len(comparison_df)), comparison_df['Model'], rotation=45, ha='right')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Plot CV scores with error bars\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.errorbar(range(len(comparison_df)), comparison_df['CV Mean'], \n",
    "             yerr=comparison_df['CV Std'], fmt='o', capsize=5)\n",
    "plt.xticks(range(len(comparison_df)), comparison_df['Model'], rotation=45, ha='right')\n",
    "plt.ylabel('Cross-Validation Score')\n",
    "plt.title('Cross-Validation Scores')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "if 'text' in best_model_name.lower() or 'naive' in best_model_name.lower() or 'svm' in best_model_name.lower():\n",
    "    y_pred_best = best_model_result['predictions']\n",
    "    y_true_best = y_test\n",
    "else:\n",
    "    y_pred_best = best_model_result['predictions']\n",
    "    y_true_best = y_struct_test\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true_best, y_pred_best)\n",
    "labels = [TRIAGE_LEVELS[i] for i in sorted(TRIAGE_LEVELS.keys())]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nDetailed Classification Report for {best_model_name}:\")\n",
    "print(classification_report(y_true_best, y_pred_best, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # For tree-based models\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Feature Importances ({best_model_name}):\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features = feature_importance.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 10 Feature Importances - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models\n",
    "    if best_model_name in text_results:\n",
    "        # Text-based model - show top TF-IDF features\n",
    "        feature_names = tfidf.get_feature_names_out()\n",
    "        coef = best_model.coef_\n",
    "        \n",
    "        for i, class_name in enumerate(labels):\n",
    "            top_indices = np.argsort(np.abs(coef[i]))[-10:]\n",
    "            top_features = [(feature_names[idx], coef[i][idx]) for idx in top_indices]\n",
    "            print(f\"\\nTop features for {class_name}:\")\n",
    "            for feature, weight in sorted(top_features, key=lambda x: abs(x[1]), reverse=True):\n",
    "                print(f\"  {feature}: {weight:.3f}\")\n",
    "    else:\n",
    "        # Structured feature model\n",
    "        coef = best_model.coef_\n",
    "        for i, class_name in enumerate(labels):\n",
    "            feature_weights = pd.DataFrame({\n",
    "                'feature': feature_columns,\n",
    "                'weight': coef[i]\n",
    "            }).sort_values('weight', key=abs, ascending=False)\n",
    "            \n",
    "            print(f\"\\nTop features for {class_name}:\")\n",
    "            print(feature_weights.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and associated components\n",
    "model_path = MODELS_DIR / \"triage_model.joblib\"\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Best model saved to: {model_path}\")\n",
    "\n",
    "# Save vectorizer if it's a text-based model\n",
    "if best_model_name in text_results:\n",
    "    vectorizer_path = MODELS_DIR / \"tfidf_vectorizer.joblib\"\n",
    "    joblib.dump(tfidf, vectorizer_path)\n",
    "    print(f\"TF-IDF vectorizer saved to: {vectorizer_path}\")\n",
    "else:\n",
    "    # Save scaler for structured models\n",
    "    scaler_path = MODELS_DIR / \"feature_scaler.joblib\"\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Feature scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(TRIAGE_LEVELS.values()))\n",
    "label_encoder_path = MODELS_DIR / \"label_encoder.joblib\"\n",
    "joblib.dump(label_encoder, label_encoder_path)\n",
    "print(f\"Label encoder saved to: {label_encoder_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'best_model': best_model_name,\n",
    "    'accuracy': best_model_result['accuracy'],\n",
    "    'cv_score': best_model_result['cv_mean'],\n",
    "    'model_type': 'text' if best_model_name in text_results else 'structured',\n",
    "    'feature_columns': feature_columns if best_model_name not in text_results else None,\n",
    "    'training_samples': len(y_train),\n",
    "    'test_samples': len(y_test)\n",
    "}\n",
    "\n",
    "import json\n",
    "metadata_path = MODELS_DIR / \"model_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"Model metadata saved to: {metadata_path}\")\n",
    "\n",
    "print(f\"\\n✅ Model training completed!\")\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Test accuracy: {best_model_result['accuracy']:.3f}\")\n",
    "print(f\"Cross-validation score: {best_model_result['cv_mean']:.3f} ± {best_model_result['cv_std']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}